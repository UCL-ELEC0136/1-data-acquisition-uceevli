{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jenv-YgSElT1"
   },
   "source": [
    "# Task 1: Small case scenario of dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZ6YahZgAZvB"
   },
   "source": [
    "## 1.1 - Create a dataset (The Tate Collection)\n",
    "As a first example, we will create our own small dataset. In particular, the dataset will consist of 10 artists which are present in the Tate Collection along with the corresponding year of birth, year of death, biological gender and number of artworks.\n",
    "\n",
    "|          Name         | Gender |   Year_Birth  |   Year_Death  | N_works |\n",
    "|-----------------------|--------|---------------|---------------|---------|\n",
    "|     Beuys, Joseph     |  Male  |      1921\t |      1986     |   588   |\n",
    "|    Constable, John    |  Male  |      1776\t |      1837\t |   249   |\n",
    "|   Daniell, William    |  Male  |      1769\t |      1837\t |   612   |\n",
    "|   Forbes, Elizabeth   | Female |      1859\t |      1912\t |   120   |\n",
    "|     Flaxman, John     |  Male  |      1755\t |      1826\t |   287   |\n",
    "|    Phillips, Thomas   |  Male  |      1770\t |      1845\t |   274   |\n",
    "| Paolozzi, Sir Eduardo |  Male  |      1924\t |      2005\t |   385   |\n",
    "|     Schendel, Mira    | Female |      1919\t |      1988\t |    3    |\n",
    "|    Turner, William    |  Male  |      1775\t |      1851\t |  1861   |\n",
    "|      Warhol, Andy     |  Male  |      1928\t |      1987\t |   272   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV1sgIOzAZvD"
   },
   "source": [
    "**_[TO DO]_**: Let's create a DataFrame named **TateDataset** with the information showed in the above table.\n",
    "\n",
    "**Note:** You can create arrays for each column and then build the DataFrame.\n",
    "Remember to import the required libraries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1603899361329,
     "user": {
      "displayName": "Sephora Madjiheurem",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgieP2Hh5x42748iuuSEnAl8BYCWGSmOzJ-ypgSaQ=s64",
      "userId": "05055971560005673839"
     },
     "user_tz": 0
    },
    "id": "NxkzhjIGAZvF"
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "FWsJFvTQB3hX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tate_dataset = \n",
      "                     Name  Gender  Year_Birth  Year_Death  N_works\n",
      "0          Beuys, Joseph    Male        1921        1986      588\n",
      "1        Constable, John    Male        1776        1837      249\n",
      "2       Daniell, William    Male        1769        1837      612\n",
      "3      Forbes, Elizabeth  Female        1859        1912      120\n",
      "4          Flaxman, John    Male        1755        1826      287\n",
      "5        Philips, Thomas    Male        1770        1845      274\n",
      "6  Paolozzi, Sir Eduardo    Male        1924        2005      385\n",
      "7         Schendel, Mira  Female        1919        1988        3\n",
      "8        Turner, William    Male        1775        1851     1861\n",
      "9           Warhol, Andy    Male        1928        1987      272\n"
     ]
    }
   ],
   "source": [
    "### TODO\n",
    "aname = [\"Beuys, Joseph\",\n",
    "        \"Constable, John\",\n",
    "        \"Daniell, William\",\n",
    "        \"Forbes, Elizabeth\",\n",
    "        \"Flaxman, John\",\n",
    "        \"Philips, Thomas\",\n",
    "        \"Paolozzi, Sir Eduardo\",\n",
    "        \"Schendel, Mira\",\n",
    "        \"Turner, William\",\n",
    "        \"Warhol, Andy\"]\n",
    "gm = \"Male\"\n",
    "gf = \"Female\"\n",
    "agender = [gm,gm,gm,gf,gm,gm,gm,gf,gm,gm]\n",
    "ayb = [1921,1776,1769,1859,1755,1770,1924,1919,1775,1928]\n",
    "ayd = [1986,1837,1837,1912,1826,1845,2005,1988,1851,1987]\n",
    "anw = [588,249,612,120,287,274,385,3,1861,272]\n",
    "#print(\"aname = \",aname)\n",
    "#print(\"agender = \",agender)\n",
    "#print(\"ayb = \",ayb)\n",
    "#print(\"ayd = \",ayd)\n",
    "#print(\"anw = \",anw)\n",
    "adata = {\"Name\":aname,\n",
    "        \"Gender\":agender,\n",
    "        \"Year_Birth\":ayb,\n",
    "        \"Year_Death\":ayd,\n",
    "        \"N_works\":anw}\n",
    "#print(\"adata = \",adata)\n",
    "\n",
    "tate_dataset = pd.DataFrame(data=adata)\n",
    "print(\"tate_dataset = \\n\",tate_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0P3DpT-VAZvO"
   },
   "source": [
    "## 1.2 - Data Acquisition (local file)\n",
    "\n",
    "Data acquisition is a process of loading and reading data from various sources. We will learn how to export and read data from a local file in different format using  **Pandas** package. \n",
    "\n",
    "A file format is a standardised way in which information is encoded to be stored in a file. Some examples of file formats are: CSV, XLSX, and PKL. \n",
    "\n",
    "Let's practice with some examples.\n",
    "\n",
    "### .CSV - Comma Separated Values\n",
    "\n",
    "CSV is one of the most popular spreadsheet file format. In this kind of file, data is stored in cells. Each cell is organized in rows and columns. A column in the spreadsheet file can have different types. \n",
    "\n",
    "Let us look at how to create a CSV file. We can export a DataFrame with the Pandas function `to_csv()`. The file will be saved in the same location of this notebook unless specified otherwise. We can name the file **TateData.csv**.\n",
    "\n",
    "**Note:** Once you have run the following cell, check the output file ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "DcFThhwRAZvP"
   },
   "outputs": [],
   "source": [
    "#tate_dataset.to_csv('tate_dataset.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54BZQJs8AZvU"
   },
   "source": [
    "To pull in the csv file, we will use the Pandas function `read_csv()`. We will import the previous file in a new DataFrame.\n",
    "\n",
    "**Note:**  Depending on where you save your notebooks, you may need to modify the location below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "PcO3epU5AZvV"
   },
   "outputs": [],
   "source": [
    "#tate_1 = pd.read_csv('tate_dataset.csv')\n",
    "#tate_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CmXNd9jAZva"
   },
   "source": [
    "When we create the .CSV file, the only parameter we have used is **index**. \n",
    "\n",
    "**[TO DO]** What happens if we set the *index* parameter to True? And if you set also the parameter **header** to False?\n",
    "Try by yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "egR7HZLaAZvb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntate_dataset.to_csv('tate_dataset_tt.csv', index=True, header=True)\\ntate_tt = pd.read_csv('tate_dataset_tt.csv')\\nprint(tate_tt)\\n\\ntate_dataset.to_csv('tate_dataset_tf.csv', index=True, header=False)\\ntate_tf = pd.read_csv('tate_dataset_tf.csv')\\nprint(tate_tf)\\n\\ntate_dataset.to_csv('tate_dataset_ft.csv', index=False, header=True)\\ntate_ft = pd.read_csv('tate_dataset_ft.csv')\\nprint(tate_ft)\\n\\ntate_dataset.to_csv('tate_dataset_ff.csv', index=False, header=False)\\ntate_ff = pd.read_csv('tate_dataset_ff.csv')\\nprint(tate_ff)\\n\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO\n",
    "\"\"\"\n",
    "tate_dataset.to_csv('tate_dataset_tt.csv', index=True, header=True)\n",
    "tate_tt = pd.read_csv('tate_dataset_tt.csv')\n",
    "print(tate_tt)\n",
    "\n",
    "tate_dataset.to_csv('tate_dataset_tf.csv', index=True, header=False)\n",
    "tate_tf = pd.read_csv('tate_dataset_tf.csv')\n",
    "print(tate_tf)\n",
    "\n",
    "tate_dataset.to_csv('tate_dataset_ft.csv', index=False, header=True)\n",
    "tate_ft = pd.read_csv('tate_dataset_ft.csv')\n",
    "print(tate_ft)\n",
    "\n",
    "tate_dataset.to_csv('tate_dataset_ff.csv', index=False, header=False)\n",
    "tate_ff = pd.read_csv('tate_dataset_ff.csv')\n",
    "print(tate_ff)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tHt7lPFAZvf"
   },
   "source": [
    "Delete the csv file now that we are done using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQLf1Y_hAZvr"
   },
   "source": [
    "### pickle — Python object serialization\n",
    "\n",
    "This data format is Python-specific. This has the advantage that there are no restrictions imposed by external standards; however it means that non-Python programs may not be able to reconstruct pickled Python objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "stKRPdBTAZvr"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tate_dataset.to_pickle('tate_dataset.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "Rc7RsQeKAZvt"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Year_Death</th>\n",
       "      <th>N_works</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beuys, Joseph</td>\n",
       "      <td>Male</td>\n",
       "      <td>1921</td>\n",
       "      <td>1986</td>\n",
       "      <td>588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Constable, John</td>\n",
       "      <td>Male</td>\n",
       "      <td>1776</td>\n",
       "      <td>1837</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Daniell, William</td>\n",
       "      <td>Male</td>\n",
       "      <td>1769</td>\n",
       "      <td>1837</td>\n",
       "      <td>612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Forbes, Elizabeth</td>\n",
       "      <td>Female</td>\n",
       "      <td>1859</td>\n",
       "      <td>1912</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flaxman, John</td>\n",
       "      <td>Male</td>\n",
       "      <td>1755</td>\n",
       "      <td>1826</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Philips, Thomas</td>\n",
       "      <td>Male</td>\n",
       "      <td>1770</td>\n",
       "      <td>1845</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Paolozzi, Sir Eduardo</td>\n",
       "      <td>Male</td>\n",
       "      <td>1924</td>\n",
       "      <td>2005</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Schendel, Mira</td>\n",
       "      <td>Female</td>\n",
       "      <td>1919</td>\n",
       "      <td>1988</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Turner, William</td>\n",
       "      <td>Male</td>\n",
       "      <td>1775</td>\n",
       "      <td>1851</td>\n",
       "      <td>1861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Warhol, Andy</td>\n",
       "      <td>Male</td>\n",
       "      <td>1928</td>\n",
       "      <td>1987</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name  Gender  Year_Birth  Year_Death  N_works\n",
       "0          Beuys, Joseph    Male        1921        1986      588\n",
       "1        Constable, John    Male        1776        1837      249\n",
       "2       Daniell, William    Male        1769        1837      612\n",
       "3      Forbes, Elizabeth  Female        1859        1912      120\n",
       "4          Flaxman, John    Male        1755        1826      287\n",
       "5        Philips, Thomas    Male        1770        1845      274\n",
       "6  Paolozzi, Sir Eduardo    Male        1924        2005      385\n",
       "7         Schendel, Mira  Female        1919        1988        3\n",
       "8        Turner, William    Male        1775        1851     1861\n",
       "9           Warhol, Andy    Male        1928        1987      272"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tate_2 = pd.read_pickle('tate_dataset.pkl')\n",
    "tate_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vHSnaOQAZvv"
   },
   "source": [
    "## 1.3 - Look at the data\n",
    "\n",
    "Now we will simply have a look at the data and make sure it is clean. \n",
    "\n",
    "### Data type\n",
    "Let's check the data type of the imported variables and the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "AV3eoT35AZvw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name          object\n",
       "Gender        object\n",
       "Year_Birth     int64\n",
       "Year_Death     int64\n",
       "N_works        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data type of the columns for Tate_1\n",
    "tate_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "ACysmIDSAZvy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name          object\n",
       "Gender        object\n",
       "Year_Birth     int64\n",
       "Year_Death     int64\n",
       "N_works        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data type of the columns for TateDataset\n",
    "tate_dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3q0dEk0AZv0"
   },
   "source": [
    "We can also display the values of a single column and check their data type as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "zfMt0x2lAZv0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Male\n",
       "1      Male\n",
       "2      Male\n",
       "3    Female\n",
       "4      Male\n",
       "5      Male\n",
       "6      Male\n",
       "7    Female\n",
       "8      Male\n",
       "9      Male\n",
       "Name: Gender, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tate_dataset['Gender']\n",
    "tate_dataset[\"Gender\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hympM9yTEuud"
   },
   "source": [
    "# Task 2: Application Programming Interfaces (APIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOGrvILtFv0K",
    "tags": []
   },
   "source": [
    "## 2.1 - Downloading climate data from the Internet\n",
    "\n",
    "We will now explore real world data, namely a dataset of climate information of 5 cities in Denmark between 1980-2018. The original raw-data was originally obtained from [National Climatic Data Center (NCDC)](https://www7.ncdc.noaa.gov/CDO/cdoselect.cmd).\n",
    "\n",
    "In particular, the selected cities in Denmark are:\n",
    "- Aalborg, \n",
    "- Aarhus, \n",
    "- Esbjerg, \n",
    "- Odense. \n",
    "- Roskilde\n",
    "\n",
    "In the following, we will download the dataset using an API instead of loading a local source file.\n",
    "\n",
    "### What is an API? \n",
    "Application Programming Interface (API) is a communication protocol between the user and the server (i.e., web server) that enables transmittion of data. The user making a request to an API server can download the desidered resources. \n",
    "\n",
    "### How to make a request in python?\n",
    "There are many different way to request data. \n",
    "In the following, we will use the package **urllib** that collects several modules for working with URLs.\n",
    "\n",
    "In particular, [urlib.request](https://docs.python.org/3/library/urllib.request.html#module-urllib.request) is a module for opening and reading URLs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiwhudMrKQUP"
   },
   "source": [
    "### - Download climate data\n",
    "\n",
    "\n",
    "The following functions allow to download and store the dataset in a specific folder.\n",
    "\n",
    "How to use:\n",
    "\n",
    "- Set the source (i.e., URL) of the desidered dataset in `data_url`.\n",
    "- Set the `data_dir` variable with the local directory where to store the data.\n",
    "- Call `download_and_extract()` to download the dataset if it is not already located in the given data_dir.\n",
    "- Load the data in the interactive Python notebook so ic can be used in your scripts (you can use `load_original_data()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "cellView": "form",
    "id": "DjVqxHOLE6I4"
   },
   "outputs": [],
   "source": [
    "#@title Functions to download data \n",
    "########################################################################\n",
    "#\n",
    "# This file is a partially modified version of one of the TensorFlow Tutorials available at:\n",
    "#\n",
    "# https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
    "#\n",
    "# Published under the MIT License. See the file LICENSE for details.\n",
    "#\n",
    "# Copyright 2018 by Magnus Erik Hvass Pedersen\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "\n",
    "\n",
    "def _print_download_progress(count, block_size, total_size):\n",
    "    \"\"\"\n",
    "    Function used for printing the download progress.\n",
    "    Used as a call-back function in maybe_download_and_extract().\n",
    "    \"\"\"\n",
    "\n",
    "    # Percentage completion.\n",
    "    pct_complete = float(count * block_size) / total_size\n",
    "\n",
    "    # Limit it because rounding errors may cause it to exceed 100%.\n",
    "    pct_complete = min(1.0, pct_complete)\n",
    "\n",
    "    # Status-message. Note the \\r which means the line should overwrite itself.\n",
    "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
    "\n",
    "    # Print it.\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def download(base_url, filename, download_dir):\n",
    "    \"\"\"\n",
    "    Download the given file if it does not already exist in the download_dir.\n",
    "    :param base_url: The internet URL without the filename.\n",
    "    :param filename: The filename that will be added to the base_url.\n",
    "    :param download_dir: Local directory for storing the file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Path for local file.\n",
    "    save_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    # Check if the file already exists, otherwise we need to download it now.\n",
    "    if not os.path.exists(save_path):\n",
    "        # Check if the download directory exists, otherwise create it.\n",
    "        if not os.path.exists(download_dir):\n",
    "            os.makedirs(download_dir)\n",
    "\n",
    "        print(\"Downloading\", filename, \"...\")\n",
    "\n",
    "        # Download the file from the internet.\n",
    "        url = base_url + filename\n",
    "        file_path, _ = urllib.request.urlretrieve(url=url,\n",
    "                                                  filename=save_path,\n",
    "                                                  reporthook=_print_download_progress)\n",
    "\n",
    "        print(\" Done!\")\n",
    "\n",
    "\n",
    "def download_and_extract(url, download_dir):\n",
    "    \"\"\"\n",
    "    Download and extract the data if it doesn't already exist.\n",
    "    Assumes the url is a tar-ball file.\n",
    "    :param url:\n",
    "        Internet URL for the tar-file to download.\n",
    "    :param download_dir:\n",
    "        Directory where the downloaded file is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filename for saving the file downloaded from the internet.\n",
    "    # Use the filename from the URL and add it to the download_dir.\n",
    "    filename = url.split('/')[-1]\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    # Check if the file already exists.\n",
    "    # If it exists then we assume it has also been extracted,\n",
    "    # otherwise we need to download and extract it now.\n",
    "    if not os.path.exists(file_path):\n",
    "        # Check if the download directory exists, otherwise create it.\n",
    "        if not os.path.exists(download_dir):\n",
    "            os.makedirs(download_dir)\n",
    "\n",
    "        # Download the file from the internet.\n",
    "        file_path, _ = urllib.request.urlretrieve(url=url,\n",
    "                                                  filename=file_path,\n",
    "                                                  reporthook=_print_download_progress)\n",
    "\n",
    "        print()\n",
    "        print(\"Download finished. Extracting files.\")\n",
    "\n",
    "        if file_path.endswith(\".zip\"):\n",
    "            # Unpack the zip-file.\n",
    "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
    "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
    "            # Unpack the tar-ball.\n",
    "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
    "\n",
    "        print(\"Done.\")\n",
    "    else:\n",
    "        print(\"Data has apparently already been downloaded and unpacked.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhAOVRKTclRR"
   },
   "source": [
    "First, we define where we want to download the dataset (you are free to choose another directory) and the URL of the dataset to be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "mPl5lGC_cjRa"
   },
   "outputs": [],
   "source": [
    "# Location of the dataset on the internet.\n",
    "data_url = \"https://github.com/Hvass-Labs/weather-denmark/raw/master/weather-denmark.tar.gz\"\n",
    "\n",
    "# Local directory where you want to download and save the dataset.\n",
    "data_dir = \"weather-data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fUv_1U7AGIS"
   },
   "source": [
    "Now, we can download the dataset into the chosen local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "6P4iL6WfAGIT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "download_and_extract(url=data_url,download_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLC9mO_RAGIY"
   },
   "source": [
    "**[TO DO]:** Check the local folder? In which format the dataset has been downloaded?\n",
    "\n",
    "**[TO DO]:** Select a format and load the dataset ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "59A_u_4CAGIY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md is not dataset file\n",
      "weather-denmark.csv is csv format\n",
      "weather-denmark.pkl is pkl format\n",
      "weather-denmark.tar.gz is tgz format\n"
     ]
    }
   ],
   "source": [
    "### TODO\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        print(file, \"is csv format\")\n",
    "    elif file.endswith(\".pkl\"):\n",
    "        print(file, \"is pkl format\")\n",
    "    elif file.endswith((\".tar.gz\",\".tgz\")):\n",
    "        print(file, \"is tgz format\")\n",
    "    else: \n",
    "        print(file, \"is not dataset file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "36clsIOhq_CR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City          object\n",
      "DateTime      object\n",
      "Temp         float64\n",
      "Pressure     float64\n",
      "WindSpeed    float64\n",
      "WindDir      float64\n",
      "dtype: object\n",
      "             City             DateTime  Temp  Pressure  WindSpeed  WindDir\n",
      "0         Aalborg  1980-03-01 00:00:00   5.0    1008.1       11.3    290.0\n",
      "1         Aalborg  1980-03-01 00:20:00   4.0       NaN        9.2    270.0\n",
      "2         Aalborg  1980-03-01 00:50:00   4.0       NaN        9.2    280.0\n",
      "3         Aalborg  1980-03-01 01:20:00   4.0       NaN        9.2    280.0\n",
      "4         Aalborg  1980-03-01 01:50:00   4.0       NaN        8.7    270.0\n",
      "...           ...                  ...   ...       ...        ...      ...\n",
      "2918145  Roskilde  2018-03-01 22:20:00  -5.0       NaN        5.1     70.0\n",
      "2918146  Roskilde  2018-03-01 22:50:00  -5.0       NaN        4.1     70.0\n",
      "2918147  Roskilde  2018-03-01 23:00:00  -5.3    1018.6        4.1     60.0\n",
      "2918148  Roskilde  2018-03-01 23:20:00  -5.0       NaN        3.6     60.0\n",
      "2918149  Roskilde  2018-03-01 23:50:00  -5.0       NaN        3.6     60.0\n",
      "\n",
      "[2918150 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "### TODO\n",
    "wd_filename = \"weather-denmark\"\n",
    "wd_data_csv = pd.read_csv(data_dir+wd_filename+\".csv\")\n",
    "print(wd_data_csv.dtypes)\n",
    "print(wd_data_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MN4_qGC4AGIc"
   },
   "source": [
    "**More advanced solution for loading dataset**:\n",
    "\n",
    "In this case, the dataset was downloaded in two different formats. \n",
    "We can define the path where the files have been stored as follows:\n",
    "- **path_original_data_pickle( )** is the location of the original data in pickle format\n",
    "- **path_original_data_csv( )** is the location the original data in .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "NO8jevNRAGIc"
   },
   "outputs": [],
   "source": [
    "def path_original_data_pickle():\n",
    "    return os.path.join(data_dir, \"weather-denmark.pkl\")\n",
    "\n",
    "def path_original_data_csv():\n",
    "    return os.path.join(data_dir, \"weather-denmark.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoNQaCyAAGIg"
   },
   "source": [
    "Now we can load the data in pickle format through the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "JHdwG7c6AGIh"
   },
   "outputs": [],
   "source": [
    "def load_original_data():\n",
    "    return pd.read_pickle(path_original_data_pickle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-XPffh3AGIj"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMCJQr_SAGIm"
   },
   "source": [
    "### - Undestanding the data\n",
    "\n",
    "**_[TO DO]_**: Have a quick overview of the downloaded dataset. \n",
    "In particular, focus on:\n",
    "\n",
    "- Understanding the variables contained in the dataframe\n",
    "- Check the DateTime for 2 cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "-_dXNZFqd6rk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types\n",
      "Temp         float64\n",
      "Pressure     float64\n",
      "WindSpeed    float64\n",
      "WindDir      float64\n",
      "dtype: object\n",
      "----------\n",
      "DatetimeIndex(['1980-03-01 00:00:00', '1980-03-01 00:20:00',\n",
      "               '1980-03-01 00:50:00', '1980-03-01 01:20:00',\n",
      "               '1980-03-01 01:50:00', '1980-03-01 02:20:00',\n",
      "               '1980-03-01 03:00:00', '1980-03-01 03:20:00',\n",
      "               '1980-03-01 04:00:00', '1980-03-01 04:01:00',\n",
      "               ...\n",
      "               '2018-03-01 21:20:00', '2018-03-01 21:50:00',\n",
      "               '2018-03-01 22:00:00', '2018-03-01 22:20:00',\n",
      "               '2018-03-01 22:30:00', '2018-03-01 22:50:00',\n",
      "               '2018-03-01 23:00:00', '2018-03-01 23:04:00',\n",
      "               '2018-03-01 23:21:00', '2018-03-01 23:50:00'],\n",
      "              dtype='datetime64[ns]', name='DateTime', length=759897, freq=None)\n",
      "----------\n",
      "1980-03-01 00:00:00\n",
      "2018-03-01 23:50:00\n",
      "----------\n",
      "DatetimeIndex(['1980-03-01 10:50:00', '1980-03-01 12:50:00',\n",
      "               '1980-03-01 13:50:00', '1980-03-01 15:50:00',\n",
      "               '1980-03-01 16:50:00', '1980-03-02 04:50:00',\n",
      "               '1980-03-02 06:50:00', '1980-03-02 07:50:00',\n",
      "               '1980-03-02 09:50:00', '1980-03-02 10:50:00',\n",
      "               ...\n",
      "               '2018-03-01 20:50:00', '2018-03-01 21:00:00',\n",
      "               '2018-03-01 21:20:00', '2018-03-01 21:50:00',\n",
      "               '2018-03-01 22:00:00', '2018-03-01 22:20:00',\n",
      "               '2018-03-01 22:50:00', '2018-03-01 23:00:00',\n",
      "               '2018-03-01 23:20:00', '2018-03-01 23:50:00'],\n",
      "              dtype='datetime64[ns]', name='DateTime', length=569852, freq=None)\n",
      "----------\n",
      "1980-03-01 10:50:00\n",
      "2018-03-01 23:50:00\n"
     ]
    }
   ],
   "source": [
    "### TODO\n",
    "print(\"Data Types\")\n",
    "print(df.dtypes)\n",
    "print(\"----------\")\n",
    "print(df.xs(\"Aalborg\").index)\n",
    "print(\"----------\")\n",
    "print(min(df.xs(\"Aalborg\").index))\n",
    "print(max(df.xs(\"Aalborg\").index))\n",
    "print(\"----------\")\n",
    "print(df.xs(\"Roskilde\").index)\n",
    "print(\"----------\")\n",
    "print(min(df.xs(\"Roskilde\").index))\n",
    "print(max(df.xs(\"Roskilde\").index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYcAldYgeOmq"
   },
   "source": [
    "## 2.2 - Interacting with a server to query a subset of some data bank\n",
    "\n",
    "We will now learn how to query a data bank to retrieve only the relevant proportion data for a task. We will be using the World Bank Data API to access World's renewable energy consumtpion data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pz0G-PnEllEE"
   },
   "source": [
    "You first need to install the World Bank API. In a terminal, run the following command:\n",
    "\n",
    "``>> pip install wbdata``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "executionInfo": {
     "elapsed": 5409,
     "status": "ok",
     "timestamp": 1603899607656,
     "user": {
      "displayName": "Sephora Madjiheurem",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgieP2Hh5x42748iuuSEnAl8BYCWGSmOzJ-ypgSaQ=s64",
      "userId": "05055971560005673839"
     },
     "user_tz": 0
    },
    "id": "pKP6WMKA3YTH",
    "outputId": "b822a83a-4f1f-4ef3-88e6-ada5dc8e99a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wbdata in c:\\users\\egat_nbi7\\anaconda3\\envs\\ucl\\lib\\site-packages (0.3.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tabulate>=0.8.5 in c:\\users\\egat_nbi7\\anaconda3\\envs\\ucl\\lib\\site-packages (from wbdata) (0.9.0)\n",
      "Requirement already satisfied: decorator>=4.0 in c:\\users\\egat_nbi7\\anaconda3\\envs\\ucl\\lib\\site-packages (from wbdata) (5.1.1)\n",
      "Requirement already satisfied: appdirs<2.0,>=1.4 in c:\\users\\egat_nbi7\\anaconda3\\envs\\ucl\\lib\\site-packages (from wbdata) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.0 in c:\\users\\egat_nbi7\\anaconda3\\envs\\ucl\\lib\\site-packages (from wbdata) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\egat_nbi7\\anaconda3\\envs\\ucl\\lib\\site-packages (from requests>=2.0->wbdata) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\egat_nbi7\\anaconda3\\envs\\ucl\\lib\\site-packages (from requests>=2.0->wbdata) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\egat_nbi7\\anaconda3\\envs\\ucl\\lib\\site-packages (from requests>=2.0->wbdata) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\egat_nbi7\\anaconda3\\envs\\ucl\\lib\\site-packages (from requests>=2.0->wbdata) (1.26.12)\n"
     ]
    }
   ],
   "source": [
    "%pip install wbdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 496,
     "status": "ok",
     "timestamp": 1603899609741,
     "user": {
      "displayName": "Sephora Madjiheurem",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgieP2Hh5x42748iuuSEnAl8BYCWGSmOzJ-ypgSaQ=s64",
      "userId": "05055971560005673839"
     },
     "user_tz": 0
    },
    "id": "TpOO5F8YkxyR"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "import wbdata as wb\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OJzq_rWmGa-"
   },
   "source": [
    "### - Query the World Bank\n",
    "\n",
    "* **_[TO DO]_**: Use the wbdata to retrieve renewable energy consumption data of all countries in 2015.\n",
    "\n",
    "Hint: [wbdata documentation](https://wbdata.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "RXwfBzicmFO3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Renewable Energy Consumption (TJ)\n",
      "country                                                      \n",
      "BES Islands                                         112.78860\n",
      "Nauru                                                 0.32988\n",
      "Niue                                                 16.63123\n",
      "Wallis and Futuna                                     0.00000\n",
      "Caucasian and Central Asia                                NaN\n",
      "...                                                       ...\n",
      "West Bank and Gaza                                 6701.55000\n",
      "Western Sahara                                        0.00000\n",
      "Yemen, Rep.                                        2433.20100\n",
      "Zambia                                           301679.80000\n",
      "Zimbabwe                                         324422.60000\n",
      "\n",
      "[259 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "### TODO\n",
    "\n",
    "#wb.get_source()\n",
    "#wb.get_indicator(source=35)\n",
    "#wb.search_countries(\"United\")\n",
    "#wb.get_data(\"3.1_RE.CONSUMPTION\")\n",
    "recon_date = (dt.datetime(2015,1,1),dt.datetime(2015,12,31))\n",
    "recon_data = wb.get_dataframe({\"3.1_RE.CONSUMPTION\" : \"Renewable Energy Consumption (TJ)\"},data_date=recon_date)\n",
    "print(recon_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FB_UE-0nSeX"
   },
   "source": [
    "### - Undestanding the data\n",
    "\n",
    "* **_[TO DO]_**: Display different data points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "TLsp11GKn8Ai"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renewable Energy Consumption (TJ)    float64\n",
      "dtype: object\n",
      "-----------\n",
      "                            Renewable Energy Consumption (TJ)\n",
      "country                                                      \n",
      "BES Islands                                         112.78860\n",
      "Nauru                                                 0.32988\n",
      "Niue                                                 16.63123\n",
      "Wallis and Futuna                                     0.00000\n",
      "Caucasian and Central Asia                                NaN\n",
      "-----------\n",
      "country\n",
      "BES Islands                      112.78860\n",
      "Nauru                              0.32988\n",
      "Niue                              16.63123\n",
      "Wallis and Futuna                  0.00000\n",
      "Caucasian and Central Asia             NaN\n",
      "                                  ...     \n",
      "Faeroe Islands                   623.25190\n",
      "Falkland Islands                  26.28797\n",
      "Fiji                            7491.43000\n",
      "Finland                       418361.20000\n",
      "France                        760300.40000\n",
      "Name: Renewable Energy Consumption (TJ), Length: 100, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Renewable Energy Consumption (TJ)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BES Islands</th>\n",
       "      <td>112.78860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nauru</th>\n",
       "      <td>0.32988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Niue</th>\n",
       "      <td>16.63123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wallis and Futuna</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caucasian and Central Asia</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Renewable Energy Consumption (TJ)\n",
       "country                                                      \n",
       "BES Islands                                         112.78860\n",
       "Nauru                                                 0.32988\n",
       "Niue                                                 16.63123\n",
       "Wallis and Futuna                                     0.00000\n",
       "Caucasian and Central Asia                                NaN"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO\n",
    "print(recon_data.dtypes)\n",
    "print(\"-----------\")\n",
    "print(recon_data.head())\n",
    "print(\"-----------\")\n",
    "print(recon_data[\"Renewable Energy Consumption (TJ)\"][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o52Lnoq5olC9"
   },
   "source": [
    "### - Exporting the data \n",
    "\n",
    "* **_[TO DO]_**: Save the retrieved renewable energy consumption data of all countries in 2015 in one .xlsx file\n",
    "* **_[TO DO]_**: Save the renewable energy consumption data from 2006 to 2016 for each European country in individual .csv files (one per country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "-k6RXRXBpduC"
   },
   "outputs": [],
   "source": [
    "### TODO\n",
    "#%pip install openpyxl\n",
    "recon_data.to_excel(\"re_consumption_2015.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO\n",
    "import requests\n",
    "import json\n",
    "\n",
    "countries_req = requests.get(\"http://api.worldbank.org/v2/country?format=json&per_page=300\")\n",
    "countries_req_stat = countries_req.json()[0]\n",
    "countries_json = countries_req.json()[1]\n",
    "#print(countries_req_stat)\n",
    "#countries_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABW Aruba\n",
      "AFE Africa Eastern and Southern\n",
      "AFG Afghanistan\n",
      "AFR Africa\n",
      "AFW Africa Western and Central\n",
      "AGO Angola\n",
      "ALB Albania\n",
      "AND Andorra\n",
      "ARB Arab World\n",
      "ARE United Arab Emirates\n",
      "ARG Argentina\n",
      "ARM Armenia\n",
      "ASM American Samoa\n",
      "ATG Antigua and Barbuda\n",
      "AUS Australia\n",
      "AUT Austria\n",
      "AZE Azerbaijan\n",
      "BDI Burundi\n",
      "BEA East Asia & Pacific (IBRD-only countries)\n",
      "BEC Europe & Central Asia (IBRD-only countries)\n",
      "BEL Belgium\n",
      "BEN Benin\n",
      "BFA Burkina Faso\n",
      "BGD Bangladesh\n",
      "BGR Bulgaria\n",
      "BHI IBRD countries classified as high income\n",
      "BHR Bahrain\n",
      "BHS Bahamas, The\n",
      "BIH Bosnia and Herzegovina\n",
      "BLA Latin America & the Caribbean (IBRD-only countries)\n",
      "BLR Belarus\n",
      "BLZ Belize\n",
      "BMN Middle East & North Africa (IBRD-only countries)\n",
      "BMU Bermuda\n",
      "BOL Bolivia\n",
      "BRA Brazil\n",
      "BRB Barbados\n",
      "BRN Brunei Darussalam\n",
      "BSS Sub-Saharan Africa (IBRD-only countries)\n",
      "BTN Bhutan\n",
      "BWA Botswana\n",
      "CAA Sub-Saharan Africa (IFC classification)\n",
      "CAF Central African Republic\n",
      "CAN Canada\n",
      "CEA East Asia and the Pacific (IFC classification)\n",
      "CEB Central Europe and the Baltics\n",
      "CEU Europe and Central Asia (IFC classification)\n",
      "CHE Switzerland\n",
      "CHI Channel Islands\n",
      "CHL Chile\n",
      "CHN China\n",
      "CIV Cote d'Ivoire\n",
      "CLA Latin America and the Caribbean (IFC classification)\n",
      "CME Middle East and North Africa (IFC classification)\n",
      "CMR Cameroon\n",
      "COD Congo, Dem. Rep.\n",
      "COG Congo, Rep.\n",
      "COL Colombia\n",
      "COM Comoros\n",
      "CPV Cabo Verde\n",
      "CRI Costa Rica\n",
      "CSA South Asia (IFC classification)\n",
      "CSS Caribbean small states\n",
      "CUB Cuba\n",
      "CUW Curacao\n",
      "CYM Cayman Islands\n",
      "CYP Cyprus\n",
      "CZE Czechia\n",
      "DEA East Asia & Pacific (IDA-eligible countries)\n",
      "DEC Europe & Central Asia (IDA-eligible countries)\n",
      "DEU Germany\n",
      "DFS IDA countries classified as Fragile Situations\n",
      "DJI Djibouti\n",
      "DLA Latin America & the Caribbean (IDA-eligible countries)\n",
      "DMA Dominica\n",
      "DMN Middle East & North Africa (IDA-eligible countries)\n",
      "DNF IDA countries not classified as Fragile Situations\n",
      "DNK Denmark\n",
      "DNS IDA countries in Sub-Saharan Africa not classified as fragile situations \n",
      "DOM Dominican Republic\n",
      "DSA South Asia (IDA-eligible countries)\n",
      "DSF IDA countries in Sub-Saharan Africa classified as fragile situations \n",
      "DSS Sub-Saharan Africa (IDA-eligible countries)\n",
      "DZA Algeria\n",
      "EAP East Asia & Pacific (excluding high income)\n",
      "EAR Early-demographic dividend\n",
      "EAS East Asia & Pacific\n",
      "ECA Europe & Central Asia (excluding high income)\n",
      "ECS Europe & Central Asia\n",
      "ECU Ecuador\n",
      "EGY Egypt, Arab Rep.\n",
      "EMU Euro area\n",
      "ERI Eritrea\n",
      "ESP Spain\n",
      "EST Estonia\n",
      "ETH Ethiopia\n",
      "EUU European Union\n",
      "FCS Fragile and conflict affected situations\n",
      "FIN Finland\n",
      "FJI Fiji\n",
      "FRA France\n",
      "FRO Faroe Islands\n",
      "FSM Micronesia, Fed. Sts.\n",
      "FXS IDA countries classified as fragile situations, excluding Sub-Saharan Africa\n",
      "GAB Gabon\n",
      "GBR United Kingdom\n",
      "GEO Georgia\n",
      "GHA Ghana\n",
      "GIB Gibraltar\n",
      "GIN Guinea\n",
      "GMB Gambia, The\n",
      "GNB Guinea-Bissau\n",
      "GNQ Equatorial Guinea\n",
      "GRC Greece\n",
      "GRD Grenada\n",
      "GRL Greenland\n",
      "GTM Guatemala\n",
      "GUM Guam\n",
      "GUY Guyana\n",
      "HIC High income\n",
      "HKG Hong Kong SAR, China\n",
      "HND Honduras\n",
      "HPC Heavily indebted poor countries (HIPC)\n",
      "HRV Croatia\n",
      "HTI Haiti\n",
      "HUN Hungary\n",
      "IBB IBRD, including blend\n",
      "IBD IBRD only\n",
      "IBT IDA & IBRD total\n",
      "IDA IDA total\n",
      "IDB IDA blend\n",
      "IDN Indonesia\n",
      "IDX IDA only\n",
      "IMN Isle of Man\n",
      "IND India\n",
      "INX Not classified\n",
      "IRL Ireland\n",
      "IRN Iran, Islamic Rep.\n",
      "IRQ Iraq\n",
      "ISL Iceland\n",
      "ISR Israel\n",
      "ITA Italy\n",
      "JAM Jamaica\n",
      "JOR Jordan\n",
      "JPN Japan\n",
      "KAZ Kazakhstan\n",
      "KEN Kenya\n",
      "KGZ Kyrgyz Republic\n",
      "KHM Cambodia\n",
      "KIR Kiribati\n",
      "KNA St. Kitts and Nevis\n",
      "KOR Korea, Rep.\n",
      "KWT Kuwait\n",
      "LAC Latin America & Caribbean (excluding high income)\n",
      "LAO Lao PDR\n",
      "LBN Lebanon\n",
      "LBR Liberia\n",
      "LBY Libya\n",
      "LCA St. Lucia\n",
      "LCN Latin America & Caribbean \n",
      "LDC Least developed countries: UN classification\n",
      "LIC Low income\n",
      "LIE Liechtenstein\n",
      "LKA Sri Lanka\n",
      "LMC Lower middle income\n",
      "LMY Low & middle income\n",
      "LSO Lesotho\n",
      "LTE Late-demographic dividend\n",
      "LTU Lithuania\n",
      "LUX Luxembourg\n",
      "LVA Latvia\n",
      "MAC Macao SAR, China\n",
      "MAF St. Martin (French part)\n",
      "MAR Morocco\n",
      "MCO Monaco\n",
      "MDA Moldova\n",
      "MDE Middle East (developing only)\n",
      "MDG Madagascar\n",
      "MDV Maldives\n",
      "MEA Middle East & North Africa\n",
      "MEX Mexico\n",
      "MHL Marshall Islands\n",
      "MIC Middle income\n",
      "MKD North Macedonia\n",
      "MLI Mali\n",
      "MLT Malta\n",
      "MMR Myanmar\n",
      "MNA Middle East & North Africa (excluding high income)\n",
      "MNE Montenegro\n",
      "MNG Mongolia\n",
      "MNP Northern Mariana Islands\n",
      "MOZ Mozambique\n",
      "MRT Mauritania\n",
      "MUS Mauritius\n",
      "MWI Malawi\n",
      "MYS Malaysia\n",
      "NAC North America\n",
      "NAF North Africa\n",
      "NAM Namibia\n",
      "NCL New Caledonia\n",
      "NER Niger\n",
      "NGA Nigeria\n",
      "NIC Nicaragua\n",
      "NLD Netherlands\n",
      "NOR Norway\n",
      "NPL Nepal\n",
      "NRS Non-resource rich Sub-Saharan Africa countries\n",
      "NRU Nauru\n",
      "NXS IDA countries not classified as fragile situations, excluding Sub-Saharan Africa\n",
      "NZL New Zealand\n",
      "OED OECD members\n",
      "OMN Oman\n",
      "OSS Other small states\n",
      "PAK Pakistan\n",
      "PAN Panama\n",
      "PER Peru\n",
      "PHL Philippines\n",
      "PLW Palau\n",
      "PNG Papua New Guinea\n",
      "POL Poland\n",
      "PRE Pre-demographic dividend\n",
      "PRI Puerto Rico\n",
      "PRK Korea, Dem. People's Rep.\n",
      "PRT Portugal\n",
      "PRY Paraguay\n",
      "PSE West Bank and Gaza\n",
      "PSS Pacific island small states\n",
      "PST Post-demographic dividend\n",
      "PYF French Polynesia\n",
      "QAT Qatar\n",
      "ROU Romania\n",
      "RRS Resource rich Sub-Saharan Africa countries\n",
      "RUS Russian Federation\n",
      "RWA Rwanda\n",
      "SAS South Asia\n",
      "SAU Saudi Arabia\n",
      "SDN Sudan\n",
      "SEN Senegal\n",
      "SGP Singapore\n",
      "SLB Solomon Islands\n",
      "SLE Sierra Leone\n",
      "SLV El Salvador\n",
      "SMR San Marino\n",
      "SOM Somalia\n",
      "SRB Serbia\n",
      "SSA Sub-Saharan Africa (excluding high income)\n",
      "SSD South Sudan\n",
      "SSF Sub-Saharan Africa \n",
      "SST Small states\n",
      "STP Sao Tome and Principe\n",
      "SUR Suriname\n",
      "SVK Slovak Republic\n",
      "SVN Slovenia\n",
      "SWE Sweden\n",
      "SWZ Eswatini\n",
      "SXM Sint Maarten (Dutch part)\n",
      "SXZ Sub-Saharan Africa excluding South Africa\n",
      "SYC Seychelles\n",
      "SYR Syrian Arab Republic\n",
      "TCA Turks and Caicos Islands\n",
      "TCD Chad\n",
      "TEA East Asia & Pacific (IDA & IBRD countries)\n",
      "TEC Europe & Central Asia (IDA & IBRD countries)\n",
      "TGO Togo\n",
      "THA Thailand\n",
      "TJK Tajikistan\n",
      "TKM Turkmenistan\n",
      "TLA Latin America & the Caribbean (IDA & IBRD countries)\n",
      "TLS Timor-Leste\n",
      "TMN Middle East & North Africa (IDA & IBRD countries)\n",
      "TON Tonga\n",
      "TSA South Asia (IDA & IBRD)\n",
      "TSS Sub-Saharan Africa (IDA & IBRD countries)\n",
      "TTO Trinidad and Tobago\n",
      "TUN Tunisia\n",
      "TUR Turkiye\n",
      "TUV Tuvalu\n",
      "TWN Taiwan, China\n",
      "TZA Tanzania\n",
      "UGA Uganda\n",
      "UKR Ukraine\n",
      "UMC Upper middle income\n",
      "URY Uruguay\n",
      "USA United States\n",
      "UZB Uzbekistan\n",
      "VCT St. Vincent and the Grenadines\n",
      "VEN Venezuela, RB\n",
      "VGB British Virgin Islands\n",
      "VIR Virgin Islands (U.S.)\n",
      "VNM Vietnam\n",
      "VUT Vanuatu\n",
      "WLD World\n",
      "WSM Samoa\n",
      "XKX Kosovo\n",
      "XZN Sub-Saharan Africa excluding South Africa and Nigeria\n",
      "YEM Yemen, Rep.\n",
      "ZAF South Africa\n",
      "ZMB Zambia\n",
      "ZWE Zimbabwe\n"
     ]
    }
   ],
   "source": [
    "for i in countries_json:\n",
    "    print(i[\"id\"],i[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALB',\n",
       " 'AND',\n",
       " 'ARM',\n",
       " 'AUT',\n",
       " 'AZE',\n",
       " 'BEL',\n",
       " 'BGR',\n",
       " 'BIH',\n",
       " 'BLR',\n",
       " 'CHE',\n",
       " 'CHI',\n",
       " 'CYP',\n",
       " 'CZE',\n",
       " 'DEU',\n",
       " 'DNK',\n",
       " 'ESP',\n",
       " 'EST',\n",
       " 'FIN',\n",
       " 'FRA',\n",
       " 'FRO',\n",
       " 'GBR',\n",
       " 'GEO',\n",
       " 'GIB',\n",
       " 'GRC',\n",
       " 'GRL',\n",
       " 'HRV',\n",
       " 'HUN',\n",
       " 'IMN',\n",
       " 'IRL',\n",
       " 'ISL',\n",
       " 'ITA',\n",
       " 'KAZ',\n",
       " 'KGZ',\n",
       " 'LIE',\n",
       " 'LTU',\n",
       " 'LUX',\n",
       " 'LVA',\n",
       " 'MCO',\n",
       " 'MDA',\n",
       " 'MKD',\n",
       " 'MNE',\n",
       " 'NLD',\n",
       " 'NOR',\n",
       " 'POL',\n",
       " 'PRT',\n",
       " 'ROU',\n",
       " 'RUS',\n",
       " 'SMR',\n",
       " 'SRB',\n",
       " 'SVK',\n",
       " 'SVN',\n",
       " 'SWE',\n",
       " 'TJK',\n",
       " 'TKM',\n",
       " 'TUR',\n",
       " 'UKR',\n",
       " 'UZB',\n",
       " 'XKX']"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO\n",
    "\n",
    "countries_eu = []\n",
    "#countries_eu_name = []\n",
    "for i in countries_json:\n",
    "    if i[\"region\"][\"id\"] == \"ECS\":\n",
    "        countries_eu.append(i[\"id\"])\n",
    "\n",
    "countries_eu\n",
    "#countries_eu_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO\n",
    "\n",
    "import wbdata as wb\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving renewable energy consumption data of ALB in csv format...\n",
      "renewable energy consumption data of ALB is saved!\n",
      "saving renewable energy consumption data of AND in csv format...\n",
      "renewable energy consumption data of AND is saved!\n",
      "saving renewable energy consumption data of ARM in csv format...\n",
      "renewable energy consumption data of ARM is saved!\n",
      "saving renewable energy consumption data of AUT in csv format...\n",
      "renewable energy consumption data of AUT is saved!\n",
      "saving renewable energy consumption data of AZE in csv format...\n",
      "renewable energy consumption data of AZE is saved!\n",
      "saving renewable energy consumption data of BEL in csv format...\n",
      "renewable energy consumption data of BEL is saved!\n",
      "saving renewable energy consumption data of BGR in csv format...\n",
      "renewable energy consumption data of BGR is saved!\n",
      "saving renewable energy consumption data of BIH in csv format...\n",
      "renewable energy consumption data of BIH is saved!\n",
      "saving renewable energy consumption data of BLR in csv format...\n",
      "renewable energy consumption data of BLR is saved!\n",
      "saving renewable energy consumption data of CHE in csv format...\n",
      "renewable energy consumption data of CHE is saved!\n",
      "saving renewable energy consumption data of CHI in csv format...\n",
      "renewable energy consumption data of CHI is saved!\n",
      "saving renewable energy consumption data of CYP in csv format...\n",
      "renewable energy consumption data of CYP is saved!\n",
      "saving renewable energy consumption data of CZE in csv format...\n",
      "renewable energy consumption data of CZE is saved!\n",
      "saving renewable energy consumption data of DEU in csv format...\n",
      "renewable energy consumption data of DEU is saved!\n",
      "saving renewable energy consumption data of DNK in csv format...\n",
      "renewable energy consumption data of DNK is saved!\n",
      "saving renewable energy consumption data of ESP in csv format...\n",
      "renewable energy consumption data of ESP is saved!\n",
      "saving renewable energy consumption data of EST in csv format...\n",
      "renewable energy consumption data of EST is saved!\n",
      "saving renewable energy consumption data of FIN in csv format...\n",
      "renewable energy consumption data of FIN is saved!\n",
      "saving renewable energy consumption data of FRA in csv format...\n",
      "renewable energy consumption data of FRA is saved!\n",
      "saving renewable energy consumption data of FRO in csv format...\n",
      "renewable energy consumption data of FRO is saved!\n",
      "saving renewable energy consumption data of GBR in csv format...\n",
      "renewable energy consumption data of GBR is saved!\n",
      "saving renewable energy consumption data of GEO in csv format...\n",
      "renewable energy consumption data of GEO is saved!\n",
      "saving renewable energy consumption data of GIB in csv format...\n",
      "renewable energy consumption data of GIB is saved!\n",
      "saving renewable energy consumption data of GRC in csv format...\n",
      "renewable energy consumption data of GRC is saved!\n",
      "saving renewable energy consumption data of GRL in csv format...\n",
      "renewable energy consumption data of GRL is saved!\n",
      "saving renewable energy consumption data of HRV in csv format...\n",
      "renewable energy consumption data of HRV is saved!\n",
      "saving renewable energy consumption data of HUN in csv format...\n",
      "renewable energy consumption data of HUN is saved!\n",
      "saving renewable energy consumption data of IMN in csv format...\n",
      "renewable energy consumption data of IMN is saved!\n",
      "saving renewable energy consumption data of IRL in csv format...\n",
      "renewable energy consumption data of IRL is saved!\n",
      "saving renewable energy consumption data of ISL in csv format...\n",
      "renewable energy consumption data of ISL is saved!\n",
      "saving renewable energy consumption data of ITA in csv format...\n",
      "renewable energy consumption data of ITA is saved!\n",
      "saving renewable energy consumption data of KAZ in csv format...\n",
      "renewable energy consumption data of KAZ is saved!\n",
      "saving renewable energy consumption data of KGZ in csv format...\n",
      "renewable energy consumption data of KGZ is saved!\n",
      "saving renewable energy consumption data of LIE in csv format...\n",
      "renewable energy consumption data of LIE is saved!\n",
      "saving renewable energy consumption data of LTU in csv format...\n",
      "renewable energy consumption data of LTU is saved!\n",
      "saving renewable energy consumption data of LUX in csv format...\n",
      "renewable energy consumption data of LUX is saved!\n",
      "saving renewable energy consumption data of LVA in csv format...\n",
      "renewable energy consumption data of LVA is saved!\n",
      "saving renewable energy consumption data of MCO in csv format...\n",
      "renewable energy consumption data of MCO is saved!\n",
      "saving renewable energy consumption data of MDA in csv format...\n",
      "renewable energy consumption data of MDA is saved!\n",
      "saving renewable energy consumption data of MKD in csv format...\n",
      "renewable energy consumption data of MKD is saved!\n",
      "saving renewable energy consumption data of MNE in csv format...\n",
      "renewable energy consumption data of MNE is saved!\n",
      "saving renewable energy consumption data of NLD in csv format...\n",
      "renewable energy consumption data of NLD is saved!\n",
      "saving renewable energy consumption data of NOR in csv format...\n",
      "renewable energy consumption data of NOR is saved!\n",
      "saving renewable energy consumption data of POL in csv format...\n",
      "renewable energy consumption data of POL is saved!\n",
      "saving renewable energy consumption data of PRT in csv format...\n",
      "renewable energy consumption data of PRT is saved!\n",
      "saving renewable energy consumption data of ROU in csv format...\n",
      "renewable energy consumption data of ROU is saved!\n",
      "saving renewable energy consumption data of RUS in csv format...\n",
      "renewable energy consumption data of RUS is saved!\n",
      "saving renewable energy consumption data of SMR in csv format...\n",
      "renewable energy consumption data of SMR is saved!\n",
      "saving renewable energy consumption data of SRB in csv format...\n",
      "renewable energy consumption data of SRB is saved!\n",
      "saving renewable energy consumption data of SVK in csv format...\n",
      "renewable energy consumption data of SVK is saved!\n",
      "saving renewable energy consumption data of SVN in csv format...\n",
      "renewable energy consumption data of SVN is saved!\n",
      "saving renewable energy consumption data of SWE in csv format...\n",
      "renewable energy consumption data of SWE is saved!\n",
      "saving renewable energy consumption data of TJK in csv format...\n",
      "renewable energy consumption data of TJK is saved!\n",
      "saving renewable energy consumption data of TKM in csv format...\n",
      "renewable energy consumption data of TKM is saved!\n",
      "saving renewable energy consumption data of TUR in csv format...\n",
      "renewable energy consumption data of TUR is saved!\n",
      "saving renewable energy consumption data of UKR in csv format...\n",
      "renewable energy consumption data of UKR is saved!\n",
      "saving renewable energy consumption data of UZB in csv format...\n",
      "renewable energy consumption data of UZB is saved!\n",
      "saving renewable energy consumption data of XKX in csv format...\n",
      "renewable energy consumption data of XKX is saved!\n"
     ]
    }
   ],
   "source": [
    "### TODO\n",
    "reconeu_date = (dt.datetime(2006,1,1),dt.datetime(2016,12,31))\n",
    "\n",
    "for i in countries_eu:\n",
    "    print(\"saving renewable energy consumption data of \"+i+\" in csv format...\")\n",
    "    reconeu_data = wb.get_dataframe({\"3.1_RE.CONSUMPTION\" : \"Renewable Energy Consumption of \"+i+\" (TJ)\"},country=i,data_date=reconeu_date)\n",
    "    #print(i,\"\\n\",reconeu_data)\n",
    "    reconeu_data.to_csv(\"re_consumption_2006-2016_\"+i+\".csv\")\n",
    "    print(\"renewable energy consumption data of \"+i+\" is saved!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LAB1.ipynb",
   "provenance": [
    {
     "file_id": "1HXjyjuj11ovY200lbVrY4ksJrv4eEphn",
     "timestamp": 1603281631972
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.7.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ff5841e6cade3f82a179581bced375547f6ea0b825d6261e6d7ef2f2bb76843"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
